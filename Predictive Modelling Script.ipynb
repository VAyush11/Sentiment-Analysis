{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ayush\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\requests\\__init__.py:91: RequestsDependencyWarning:\n",
      "\n",
      "urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import Speller\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import cufflinks as cf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D, SimpleRNN\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"df_final.csv\")\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True) #Got an extra unwanted column and removed it\n",
    "df=df[df['Review_Clean'].notnull()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Review_Clean']\n",
    "Y=df['Reviewer_Score_Cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3,random_state=1000,stratify=Y)\n",
    "X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain_corpus=[]\n",
    "for i in X_train:\n",
    "    Xtrain_corpus.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(Xtrain_corpus)        \n",
    "        \n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest_corpus=[]\n",
    "for i in X_test:\n",
    "    Xtest_corpus.append(i)\n",
    "\n",
    "X_test_counts = count_vect.transform(Xtest_corpus)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logregmodel = linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "logregmodel.fit(X_train_tfidf, Y_train)\n",
    "pred=logregmodel.predict(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(Y_train, pred)\n",
    "print('Logistic Regression Train Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm)) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=logregmodel.predict(X_test_tfidf)\n",
    "cm = confusion_matrix(Y_test, pred)\n",
    "print('Logistic Regression Test Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tune Penalty\n",
    "scores=[]\n",
    "labels=[]\n",
    "for i in [0,0.0001,0.001,0.01,0.1,1]:\n",
    "    logregmodel=linear_model.LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=i)\n",
    "    cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    score=cross_val_score(logregmodel, X_train_tfidf, Y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    scores.append(score)\n",
    "    labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores, labels=labels, showmeans=True)\n",
    "plt.show()\n",
    "#C=1 yields the best result and hence the model remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodel = DecisionTreeClassifier()\n",
    "dtmodel.fit(X_train_tfidf,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dtmodel.predict(X_train_tfidf)\n",
    "cm = confusion_matrix(Y_train, pred)\n",
    "print('Decision Tree Train Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm)) \n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dtmodel.predict(X_test_tfidf)\n",
    "cm = confusion_matrix(Y_test, pred)\n",
    "print('Decision Tree Test Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm)) \n",
    "print(accuracy)\n",
    "\n",
    "#Clearly, the model has overfit the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodel.get_depth() #No wonder it overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimising hyperparameters for the decision tree model\n",
    "\n",
    "var_max_depth = []\n",
    "var_criterion = []\n",
    "var_ccp_alpha = []\n",
    "trainset_accuracy = []\n",
    "testset_accuracy = []\n",
    "\n",
    "for i in range(5, 150, 3):\n",
    "    for j in ['gini', 'entropy']:\n",
    "        for k in [0, 0.0001, 0.001, 0.01, 0.1, 1]:\n",
    "            model = DecisionTreeClassifier(max_depth = i, criterion = j, ccp_alpha = k)\n",
    "            m1 = model.fit(X_train_tfidf,Y_train)\n",
    "            pred = m1.predict(X_train_tfidf)\n",
    "            cm = confusion_matrix(Y_train, pred)\n",
    "            print(\"Parameters: \", i, j, k)\n",
    "            print('Decision Tree Train Accuracy')\n",
    "            accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm)) \n",
    "            print(accuracy)\n",
    "            trainset_accuracy.append(accuracy)\n",
    "\n",
    "            pred = m1.predict(X_test_tfidf)\n",
    "            cm = confusion_matrix(Y_test, pred)\n",
    "            print('Decision Tree Test Accuracy')\n",
    "            accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm)) \n",
    "            print(accuracy)\n",
    "            print(\"=========\")\n",
    "            print()\n",
    "            testset_accuracy.append(accuracy)\n",
    "            \n",
    "            var_max_depth.append(i)\n",
    "            var_criterion.append(j)\n",
    "            var_ccp_alpha.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dt = pd.DataFrame(list(zip(var_max_depth, var_criterion, var_ccp_alpha, trainset_accuracy, testset_accuracy)), columns = ['Max_Depth', 'Criterion', 'CCP_Alpha', 'Trainset_Acc', 'Testset_Acc'])\n",
    "df_dt['Difference'] = abs(df_dt['Trainset_Acc'] - df_dt['Testset_Acc'])\n",
    "df_dt[df_dt.Testset_Acc == df_dt.Testset_Acc.max()]\n",
    "\n",
    "#We can choose the model that gives us the lowest difference betwene trainset and testset accuracies (As it is generalising to unseen data better), and within it, the highest accuracy figures\n",
    "#We can see that this yeilds an optimal model with multiple max_depth levels (As they all provide same accuracy metrics)\n",
    "#Taking the lowest depth value, we get a model with a max_depth of 23 and testset accuracy of 0.592516"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbmodel=MultinomialNB()\n",
    "nbmodel.fit(X_train_tfidf, Y_train)\n",
    "pred = nbmodel.predict(X_train_tfidf)\n",
    "cm = confusion_matrix(Y_train, pred)\n",
    "print('Naives Bayes Multinomial Train Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = nbmodel.predict(X_test_tfidf)\n",
    "cm = confusion_matrix(Y_test, pred)\n",
    "print('Naives Bayes Multinomial Test Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel=GradientBoostingClassifier()\n",
    "xgbmodel.fit(X_train_tfidf, Y_train)\n",
    "pred = xgbmodel.predict(X_train_tfidf)\n",
    "cm=confusion_matrix(Y_train, pred)\n",
    "print('XGBoost Train Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgbmodel.predict(X_test_tfidf)\n",
    "cm=confusion_matrix(Y_test, pred)\n",
    "print('XGBoost Test Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimisation\n",
    "X_train1,X_validate,Y_train1,Y_validate=train_test_split(X_train_tfidf,Y_train,test_size=0.2,random_state=1000,stratify=Y_train)\n",
    "X_train1,X_validate,Y_train1,Y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimisation\n",
    "for i in range(20):\n",
    "    print(i)\n",
    "    model = GradientBoostingClassifier(max_depth=i+1)\n",
    "    model.fit(X_train1, Y_train1)\n",
    "    pred = model.predict(X_validate)\n",
    "    cm = confusion_matrix(Y_validate, pred)\n",
    "    accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "    print(accuracy)\n",
    "    print(\"===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbmodel=GradientBoostingClassifier(max_depth=14)\n",
    "xgbmodel.fit(X_train_tfidf, Y_train)\n",
    "pred = xgbmodel.predict(X_train_tfidf)\n",
    "cm=confusion_matrix(Y_train, pred)\n",
    "print('XGBoost Train Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=xgbmodel.predict(X_test_tfidf)\n",
    "cm=confusion_matrix(Y_test, pred)\n",
    "print('XGBoost Test Accuracy')\n",
    "print(cm)\n",
    "accuracy = (cm[0,0]+cm[1,1]+cm[2,2])/sum(sum(cm))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 15247)), #input shape = 1,15247\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation = 'sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "# Create a learning rate scheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/10)) # traverse a set of learning rate values starting from 1e-4, increasing by 10**(epoch/2) every 10 epoch\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_tfidf.toarray(),Y_train, epochs=50, callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout the history\n",
    "pd.DataFrame(history.history).plot(figsize=(10,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning rate versus the loss\n",
    "lrs = 1e-4 * (10 ** (np.arange(50)/10))\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.semilogx(lrs, history.history[\"loss\"]) # we want the x-axis (learning rate) to be log scale\n",
    "plt.xlabel(\"Learning Rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Learning rate vs. loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in this graph, loss of the NN was the lowest when the learning rate was between the 10^-3 to 10^-1 range\n",
    "\n",
    "We will use a learning rate of 10^-3 as a large learning rate of 10^-2  can cause the model to converge too quickly to a suboptimal solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modified NN Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model_2 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 15247)), #input shape = 1,15247\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dense(4, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model_2.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_2 = model_2.fit(X_train_tfidf.toarray(),Y_train,batch_size = 5, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkout the history\n",
    "pd.DataFrame(history_2.history).plot(figsize=(10,7));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_3 = model_2.fit(X_train_tfidf.toarray(),Y_train,batch_size = 5, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model_3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 15247)), #input shape = 1,15247\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model_3.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_4 = model_3.fit(X_train_tfidf.toarray(),Y_train,batch_size = 5, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model_4 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 15247)), #input shape = 1,15247\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model_4.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_4.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_4 = model_4.fit(X_train_tfidf.toarray(),Y_train, batch_size = 5, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_6 = model_4.fit(X_train_tfidf.toarray(),Y_train, batch_size = 10, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_7 = model_4.fit(X_train_tfidf.toarray(),Y_train, batch_size = 5, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_7 = model_4.fit(X_train_tfidf.toarray(),Y_train, batch_size = 10, epochs = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model_5 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 15247)), #input shape = 1,15247\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation = 'relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model_5.compile(optimizer= tf.keras.optimizers.Adamax(learning_rate=0.001),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_5.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_8 = model_5.fit(X_train_tfidf.toarray(),Y_train, batch_size = 5, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "model_6 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 15247)), #input shape = 1,15247\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(4, activation = 'sigmoid'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax')\n",
    "])\n",
    "model_6.compile(optimizer= tf.keras.optimizers.Adamax(learning_rate=0.001),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model_6.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with lower number of epoches based on the loss and accuracy curves of previous graph\n",
    "history_9 = model_6.fit(X_train_tfidf.toarray(),Y_train, batch_size = 5, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_6.evaluate(X_test_tfidf.toarray(), Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that models 5 and 6 perform best; we can select model 6 as the 'ideal' ANN model, as it offers a low difference between trainset and testset accuracies, and offers a reasonably high testset accuracy of 63.5%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
