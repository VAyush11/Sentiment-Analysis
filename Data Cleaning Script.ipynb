{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has 515738 rows and 17 columns\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Hotel_Reviews.csv\")\n",
    "print(f\"Dataset has {df.shape[0]} rows and {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the various tags into independent columns so that we can get maximum information and can use these features in modelling\n",
    "new = df[\"Tags\"].str.split(\",\", n = -1, expand = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up the separated 'Tags' dataframe first, will then merge it back to our dataset\n",
    "\n",
    "#Naming the new column names for easier reference\n",
    "new = new.set_axis(['Tag1', 'Tag2', 'Tag3', 'Tag4', 'Tag5', 'Tag6'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning column contents\n",
    "\n",
    "#Removing unwanted special characters, such as ', [, and ]\n",
    "for column in new:\n",
    "    new[column] = new[column].str.replace(\"'\", \"\")\n",
    "for column in new:\n",
    "    new[column] = new[column].str.replace(\"[\", \"\")\n",
    "for column in new:\n",
    "    new[column] = new[column].str.replace(\"]\", \"\")\n",
    "    \n",
    "#Removing unwanted leading and trailing spaces within each column\n",
    "#new['Tag1'] = new['Tag1'].str.strip()\n",
    "for column in new:\n",
    "    new[column] = new[column].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "416672"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag1leisure = new.index[new['Tag1'] == 'Leisure trip'].tolist()\n",
    "\n",
    "#We will have to remove rows from the original df systematically\n",
    "#First we keep only leisure trip rows, and only then remove non-child travellers from the remaining rows \n",
    "indexestokeep = tag1leisure\n",
    "len(indexestokeep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking what information is present in each tag\n",
    "new.Tag1.unique() #Type of trip\n",
    "new.Tag2.unique() #Type of trip (Overlap with Tag1) & Room Type\n",
    "new.Tag3.unique() #Type of Room (Overlap with Tag2)\n",
    "new.Tag4.unique() #Type of Room (Overlap with Tag2 & Tag3) & Length of Stay\n",
    "new.Tag5.unique() #Length of Stay (Overlap with Tag4)\n",
    "new.Tag6.unique() #Submitted from a mobile device\n",
    "\n",
    "#There's a considerable amount of overlap in the information the tags are offering, and this will be particularly difficult to clean\n",
    "#We should aim to find some pattern which standardises the information offered, as a lot of the information offered is 'off' by 1 column\n",
    "\n",
    "#Checking to see the frequency of each category in the columns\n",
    "new['Tag1'].value_counts()\n",
    "#Clearly, over 498k rows have 'leisure' or 'business' type of travel stated, and these are the only 2 categories not included in Tag2, suggesting that rows without this information 'Start from Tag1', and rows with this additional information 'Start from Tag2'\n",
    "#Thus, keeping or removing rows with these values is the key to standardising the dataset\n",
    "\n",
    "#The value counts shows that 498k rows have this demarkation, and rows that don't have them are in the minority (17k)\n",
    "#Thus, we will keep the rows with this distinction, and drop the ones that don't have this\n",
    "\n",
    "new4 = new.loc[indexestokeep]\n",
    "\n",
    "#Resetting index to ensure we get the accurate indexes for filtering by traveller_category == family\n",
    "new4 = new4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating the filtering process to filter by travller_category == family\n",
    "tag2familieswithyoungkids = new4.index[new4['Tag2'] == 'Family with young children'].tolist()\n",
    "tag2familieswitholdkids = new4.index[new4['Tag2'] == 'Family with older children'].tolist()\n",
    "\n",
    "indexestokeep1 = tag2familieswithyoungkids + tag2familieswitholdkids\n",
    "new4 = new4.loc[indexestokeep1]\n",
    "\n",
    "new4.Tag1.unique() #Type of trip\n",
    "new4.Tag2.unique() #Who's taking the trip (Travellers type)\n",
    "new4.Tag3.unique().tolist() #Room Type - the tolist allowed me to view all the unique elements at once\n",
    "new4.Tag4.unique() #Length of stay\n",
    "new4.Tag5.unique() #Whether or not submitted from mobile device\n",
    "new4.Tag6.unique() #None\n",
    "\n",
    "new4 = new4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80946, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new4.shape #(499420, 6) <-- This shows that only the relevant rows were kept\n",
    "\n",
    "#We also need to remove these rows from the original dataframe to ensure that once we clean up the tags and merge the tags df back to the original, the tags are assigned to the appropriate rows\n",
    "df = df.loc[indexestokeep]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df = df.loc[indexestokeep1]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.shape #(80946, 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With unwanted characters removed, we can create dummy variables for the various categories listed here, and then merge this back to the original dataframe\n",
    "\n",
    "#Renaming the columns to reflect the information they capture\n",
    "new4['Tag1'].unique()\n",
    "new4 = new4.rename(columns = {'Tag1': 'Travel_Type'})\n",
    "\n",
    "new4['Tag2'].unique()\n",
    "new4 = new4.rename(columns = {'Tag2': 'Traveller_Category'})\n",
    "\n",
    "new4['Tag3'].unique()\n",
    "new4 = new4.rename(columns = {'Tag3': 'Room_Type'})\n",
    "\n",
    "new4['Tag4'].unique()\n",
    "new4 = new4.rename(columns = {'Tag4': 'Stay_Length'})\n",
    "\n",
    "new4['Tag5'].unique()\n",
    "new4 = new4.rename(columns = {'Tag5': 'Submitted_From_Mobile'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Narrowing down the dataset to leisure trips by families with children (both old and young)\n",
    "new4 = new4[new4.Travel_Type == 'Leisure trip']\n",
    "\n",
    "new4 = new4[(new4.Traveller_Category == 'Family with young children') | (new4.Traveller_Category == 'Family with older children')]\n",
    "new4_dict = new4['Room_Type'].value_counts(normalize = True).to_dict() #Filter out values less than 10\n",
    "\n",
    "new4['Value_Counts_Percent'] = new4['Room_Type']\n",
    "new4['Value_Counts_Percent'] = new4['Value_Counts_Percent'].map(new4_dict)\n",
    "\n",
    "#Using only those categories that represent at least 1% of the overall dataset\n",
    "new4 = new4[new4.Value_Counts_Percent > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating the filtering process to filter by the value_counts threshold\n",
    "new4 = new4.reset_index(drop=True)\n",
    "\n",
    "requiredvaluecounts = new4.index[new4['Value_Counts_Percent'] > 0.01].tolist()\n",
    "new4 = new4.loc[requiredvaluecounts]\n",
    "\n",
    "#Similarly cleaning the original df to ensure consistency\n",
    "df = df.loc[requiredvaluecounts]\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further cleaning up the 'new4' dataset so that processing is easy, after which we can easily merge it back to the original df\n",
    "\n",
    "#Categorising Submitted_From_Mobile into (1,0) classification\n",
    "new4['Submitted_From_Mobile'] = new4['Submitted_From_Mobile'].replace(\"Submitted from a mobile device\", 1)\n",
    "new4['Submitted_From_Mobile'] = new4['Submitted_From_Mobile'].fillna(0)\n",
    "new4\n",
    "\n",
    "#Removing Tag6 as it is completely empty\n",
    "new4 = new4.drop('Tag6', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the categorical 'Stay_Length' column into continuous\n",
    "new4['Stay_Length'] = new4.Stay_Length.str.extract('(\\d+)')\n",
    "new4.dtypes\n",
    "\n",
    "#The data-type is still string, and we need to convert this to numeric\n",
    "new4.Stay_Length = pd.to_numeric(new4.Stay_Length, errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting dummies for each of the variables in new4 to allow ML models to process the data\n",
    "#We need to get dummies for only 3 columns - Travel_Type, Traveller_Category, and Room_Type\n",
    "\n",
    "#Will use one-hot encoding\n",
    "ohe_Travel_Type = pd.get_dummies(new4['Travel_Type'])\n",
    "ohe_Traveller_Category = pd.get_dummies(new4['Traveller_Category'])\n",
    "ohe_Room_Type = pd.get_dummies(new4['Room_Type'])\n",
    "\n",
    "#We can now merge the one_hot_encoded columns into new4 and drop the original columns\n",
    "new4 = new4.join([ohe_Travel_Type, ohe_Traveller_Category, ohe_Room_Type])\n",
    "new4.drop(['Travel_Type', 'Traveller_Category', 'Room_Type'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will now merge the cleaned Tags to the original dataset and drop the original 'Tags' column\n",
    "df.drop('Tags', axis = 1, inplace = True)\n",
    "df = df.join(new4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Exploration.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can now proceed to further clean the dataset as a whole\n",
    "\n",
    "df.isna().sum()\n",
    "#Latitude & Longitude are the only columns with na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Additional_Number_of_Scoring', 'Review_Date', 'days_since_review', 'lat', 'lng'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New column to consolidate the negative and positive reviews\n",
    "\n",
    "df[\"Review\"] = df[\"Negative_Review\"] + df[\"Positive_Review\"]\n",
    "df.drop(['Negative_Review', 'Positive_Review'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'No Negative' & 'No Positive' text from column bececause user did not give negative and positive reviews\n",
    "\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda x: x.replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenisation process and text cleaning\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(pos_tag):\n",
    "    if pos_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "import string\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def clean_text(text):\n",
    "    # lower text\n",
    "    text = text.lower()\n",
    "    # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove stop words\n",
    "    stop = stopwords.words('english')\n",
    "    text = [x for x in text if x not in stop]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # pos tag text\n",
    "    pos_tags = pos_tag(text)\n",
    "    # lemmatize text\n",
    "    text = [WordNetLemmatizer().lemmatize(t[0], get_wordnet_pos(t[1])) for t in pos_tags]\n",
    "    # remove words with only one letter\n",
    "    text = [t for t in text if len(t) > 1]\n",
    "    # join all\n",
    "    text = \" \".join(text)\n",
    "    return(text)\n",
    "\n",
    "# clean text data\n",
    "df[\"Review_Clean\"] = df[\"Review\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "docs=df['Review_Clean'].tolist()\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.95)\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector\n",
    "\n",
    "#Now, let's look at 10 words from our vocabulary. Sweet, these are mostly crime and comedy related.\n",
    "list(cv.vocabulary_.keys())[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Important keywords extraction using tfidf\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "                stop_words='english',\n",
    "                use_idf=True,\n",
    "                lowercase=True,\n",
    "                smooth_idf=True) \n",
    "# tfidf_transformer = vectorizer.fit_transform(df.Review_Clean)\n",
    "\n",
    "\n",
    "# print(df.Review_Clean)\n",
    "# vector = cv.transform(df.Review_Clean[1])\n",
    "tfidf_vector = vectorizer.fit_transform(df.Review_Clean)  # tfidf_transformer.transform(vector)\n",
    "coo_matrix = tfidf_vector.tocoo()\n",
    "tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "sorted_tuple = sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "scores = pd.DataFrame([(cv.get_feature_names()[i[0]],i[1]) for i in sorted_tuple])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = scores.rename(columns = {0: 'word', 1: 'score'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.word.nunique() #18041"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the average score for each word\n",
    "scores_mean = scores.groupby('word').mean()\n",
    "scores_mean = scores_mean.reset_index()\n",
    "words = scores_mean.word.tolist()\n",
    "scores = scores_mean.score.tolist()\n",
    "scores_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correcting misspelt words identified and scored by tfidf\n",
    "from autocorrect import Speller\n",
    "spell = Speller(lang = 'en')\n",
    "\n",
    "corrected_words = []\n",
    "\n",
    "for each in words:\n",
    "    each = spell(each)\n",
    "    corrected_words.append(each)\n",
    "\n",
    "#Appending the corrected words to the df with the scores as a separate column\n",
    "scores_mean = scores_mean.assign(Corrected_Words = corrected_words)\n",
    "\n",
    "#True and False were mis-identified and their correct spelling corrupted. Restoring their original spellings in lowercase \n",
    "scores_mean.Corrected_Words.replace(\"FAS\", \"false\", inplace = True)\n",
    "scores_mean.Corrected_Words.replace(\"RE\", \"true\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking the average score for each corrected word. As several words post-correction would be identical but with their earlier scores (Ex: Worlde --> world)  \n",
    "scores_mean = scores_mean.groupby('Corrected_Words').mean()\n",
    "scores_mean = scores_mean.reset_index()\n",
    "words = scores_mean.Corrected_Words.tolist()\n",
    "score = scores_mean.score.tolist()\n",
    "scores_dict = dict(zip(words, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning out problematic values\n",
    "\n",
    "#Starting with na values for cleaned review which cannot have a tfidf score attached to them\n",
    "df_new = df[df['Review_Clean'].notnull()]\n",
    "\n",
    "#Also editing out row 871, which is 'missing'(Despite resetting index) and causes problems in tdidf score computation for each individual review\n",
    "df.new1 = df_new[0:870]\n",
    "df.new2 = df_new[872:38870]\n",
    "frames = [df.new1, df.new2]\n",
    "df_final = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running a for-loop to calculate the tfidf scores for each review (row), which is the sum of the tfidf scores of the individual words used to compile the review\n",
    "\n",
    "tfidf_score = []\n",
    "score = 0\n",
    "count = 0\n",
    "for each_review in df_final['Review_Clean']:\n",
    "    words_in_review = each_review.split()\n",
    "    for each_word in words_in_review:\n",
    "        if each_word in scores_dict:\n",
    "            score += scores_dict[each_word]\n",
    "        else:\n",
    "            pass\n",
    "    tfidf_score.append(score)\n",
    "    score = 0\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.assign(tfidf_score = tfidf_score)\n",
    "#df_final.drop('Unnamed: 0', axis = 1, inplace = True) <-- Got an extra unwanted column and removed it\n",
    "df_final.to_csv(\"df_final.csv\") #This is the final, cleaned dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
